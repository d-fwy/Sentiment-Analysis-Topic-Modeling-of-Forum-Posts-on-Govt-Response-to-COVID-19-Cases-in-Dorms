{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Prediction Modeling\n",
    "\n",
    "### Determining production model: Evaluating performance of LSTM RNN model, Logistic Regression model, Multinomial Naive Bayes model to determine which is the best model\n",
    "\n",
    "Having gathered the posts by scraping, a sample of 1000 posts were labelled manually so as to train models to subsequently predict labels on the remainder of the dataset (around 9000 posts). *Textblob and VADER (unsupervised learning text classification models) were initially used to predict the sentiment of the posts, but the results were not accurate*\n",
    "\n",
    "The labels are: \n",
    "- **0 for negative sentiment** \n",
    "- **1 for neutral sentiment**\n",
    "- **2 for positive sentiment**\n",
    "\n",
    "It should be noted that the labels were highly skewed, with around 80% of the posts classed as neutral, 15% as negative and 5% positive. This affected how well the model was able to predict the sentiment of unlabelled posts. \n",
    "\n",
    "The models chosen were classification models - Logistic Regression and Multinomial Naive Bayes, Long Short Term Memory Recurrent Neural Net model and BERT. \n",
    "\n",
    "It was found that the Multinomial Naive Bayes model was the most accurate in predicting sentiment. It had high Accuracy, ROC AUC and F1 scores, with little variance between the train and validation sets, as well as being more likely to assign minority classes, compared to the Logistic Regression Model. The performance of LSTM RNN and BERT models paled in comparison as well. For BERT, it could be that the pretrained model did not generalise well on the dataset, due to the nature of the local Singlish language (in terms of different words and sentence structures). The LSTM RNN model also did not do well, as it relied on learning words before and after a significant word, instead of standalone words, which did not work particularly well for this dataset.  \n",
    "\n",
    "It should be noted that methods such as SMOTE, random oversampling and manipulating class weights were used to try to address the issue of unbalanced classes in the data, all of which did not perform as well as a model where the minority classes were not oversampled/given more weightage in the model. \n",
    "\n",
    "The process of labelling posts using a Supervised Learning model went as follows:\n",
    "1. Split all posts into train (8214 posts) and test (2056 posts) sets \n",
    "2. From train set, manually label sentiment of first 1000 posts \n",
    "3. Trained first 1000 labelled posts on Multinomial NB model \n",
    "4. Predict label for next 1000 unlabelled posts \n",
    "5. Check for accuracy and make changes to incorrect labels \n",
    "6. Collate labelled posts and train model  \n",
    "7. Label subsequent 1000 posts \n",
    "8. Repeat steps 3 to 7 until all of train posts (8214 posts) have been labelled \n",
    "9. Train Multinomial NB model on all posts in train set, which have been labelled \n",
    "10. Predict label on posts in test set \n",
    "11. Check accuracy of predictions and make changes to incorrect labels \n",
    "12. Collate labelled posts in train and test sets to train final production model **(THIS NOTEBOOK)** \n",
    "\n",
    "While I acknowledge that it is not ideal to use a regular supervised learning classification models to predict the sentiment of text posts, this was the best method for this particular dataset. I believe that with more data gathered, a LSTM RNN or BERT model can be sufficiently trained to better predict sentiment of posts from Singaporean forums.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Importing of libraries, train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding, SpatialDropout1D, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_for_rnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>There is no new cluster beside the dorm for tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-10-04 08:34:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>there is no new cluster beside the dorm for tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&gt;For consistency and accuracy, it could be eas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-11 12:26:38</td>\n",
       "      <td>reddit</td>\n",
       "      <td>for consistency and accuracy it could be easie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Only IQ lower than 86 will believe this CSB.Wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-23 10:42:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>only iq lower than 86 will believe this csb wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I work nearby to the Westlite and Toh Guan Dor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-06 20:55:47</td>\n",
       "      <td>reddit</td>\n",
       "      <td>i work nearby to the westlite and toh guan dor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ho seh liao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-04 21:43:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>ho seh liao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               post  label  \\\n",
       "0           0  There is no new cluster beside the dorm for tw...    2.0   \n",
       "1           1  >For consistency and accuracy, it could be eas...    0.0   \n",
       "2           2  Only IQ lower than 86 will believe this CSB.Wh...    0.0   \n",
       "3           3  I work nearby to the Westlite and Toh Guan Dor...    0.0   \n",
       "4           4                                        Ho seh liao    0.0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-10-04 08:34:00  hardwarezone   \n",
       "1  2020-04-11 12:26:38        reddit   \n",
       "2  2020-04-23 10:42:00  hardwarezone   \n",
       "3  2020-04-06 20:55:47        reddit   \n",
       "4  2020-09-04 21:43:00  hardwarezone   \n",
       "\n",
       "                                  post_clean_for_rnn  \n",
       "0  there is no new cluster beside the dorm for tw...  \n",
       "1  for consistency and accuracy it could be easie...  \n",
       "2  only iq lower than 86 will believe this csb wh...  \n",
       "3  i work nearby to the westlite and toh guan dor...  \n",
       "4                                        ho seh liao  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./labelled_posts/train_labelled_clean.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post                  0\n",
       "label                 0\n",
       "date                  0\n",
       "source                0\n",
       "post_clean_for_rnn    5\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_for_rnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no new cluster beside the dorm for tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-10-04 08:34:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>there is no new cluster beside the dorm for tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;For consistency and accuracy, it could be eas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-11 12:26:38</td>\n",
       "      <td>reddit</td>\n",
       "      <td>for consistency and accuracy it could be easie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0  There is no new cluster beside the dorm for tw...    2.0   \n",
       "1  >For consistency and accuracy, it could be eas...    0.0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-10-04 08:34:00  hardwarezone   \n",
       "1  2020-04-11 12:26:38        reddit   \n",
       "\n",
       "                                  post_clean_for_rnn  \n",
       "0  there is no new cluster beside the dorm for tw...  \n",
       "1  for consistency and accuracy it could be easie...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "display(train.isnull().sum())\n",
    "train.fillna('nopost', inplace=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need more camp to house all foreign worker</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-15 15:09:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>need camp house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im interested in the clusters esp new ones. Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-29 23:07:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>im interested clusters esp new ones see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did the virus make his kkj buay kia?</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-23 21:22:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>make kkj buay kia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m not saying there isn’t room for improvemen...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-08 11:23:23</td>\n",
       "      <td>reddit</td>\n",
       "      <td>not saying room improvement point choice nsfs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnn good life no need work get paid and free ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-05 13:04:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>abnn good life no need work get paid free food...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0         Need more camp to house all foreign worker      1   \n",
       "1  Im interested in the clusters esp new ones. Wh...      1   \n",
       "2               did the virus make his kkj buay kia?      1   \n",
       "3  I’m not saying there isn’t room for improvemen...      1   \n",
       "4  abnn good life no need work get paid and free ...      1   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-04-15 15:09:00  hardwarezone   \n",
       "1  2020-04-29 23:07:00  hardwarezone   \n",
       "2  2020-04-23 21:22:00  hardwarezone   \n",
       "3  2020-04-08 11:23:23        reddit   \n",
       "4  2020-04-05 13:04:00  hardwarezone   \n",
       "\n",
       "                                          post_clean  \n",
       "0                                    need camp house  \n",
       "1            im interested clusters esp new ones see  \n",
       "2                                  make kkj buay kia  \n",
       "3  not saying room improvement point choice nsfs ...  \n",
       "4  abnn good life no need work get paid free food...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./labelled_posts/test_posts_labelled_checked.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post          0\n",
       "label         0\n",
       "date          0\n",
       "source        0\n",
       "post_clean    8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need more camp to house all foreign worker</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-15 15:09:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>need camp house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im interested in the clusters esp new ones. Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-29 23:07:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>im interested clusters esp new ones see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0         Need more camp to house all foreign worker      1   \n",
       "1  Im interested in the clusters esp new ones. Wh...      1   \n",
       "\n",
       "                  date        source                               post_clean  \n",
       "0  2020-04-15 15:09:00  hardwarezone                          need camp house  \n",
       "1  2020-04-29 23:07:00  hardwarezone  im interested clusters esp new ones see  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(test.isnull().sum())\n",
    "test.fillna('nopost', inplace=True)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing of posts in train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing train posts - for classification models, logreg and multinomial naive bayes \n",
    "\n",
    "Preprocessing for RNN model already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dealing with stopwords \n",
    "\n",
    "#first cut - removing words from stopwords that indicate sentiment \n",
    "remove_words = [\"no\", \"not\", \"against\", \"don't\", \"should\", \"should've\", \"couldn\", \"couldn't\",'didn', \"didn't\",\n",
    "                   'doesn',\"doesn't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",\n",
    "                   'wouldn',\"wouldn't\"]\n",
    "stopwords = [word for word in stopwords.words('english') if word not in remove_words]\n",
    "print(len(stopwords))\n",
    "\n",
    "#also adding words that are either common words, singaporean slang or noisy words from forum posts\n",
    "add_words = ['foreign', 'migrant', 'worker', 'workers', 'fw', 'dorm', 'dorms', 'dormitory', 'dormitories', 'covid', \n",
    "             '19', 'cases', 'virus', 'coronavirus', 'gagt', 'ah', 'liao', 'lah', 'trt', 'huawei', 'samsung',\n",
    "            'xiaomi', 'l21a', '32']\n",
    "stopwords.extend(add_words)\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(word):\n",
    "     \n",
    "    # tokenize and convert lower \n",
    "    # \\w also removes punctuation - may need to add extra no punc if tokenizing does not do it\n",
    "    token = RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(word.lower())\n",
    "    \n",
    "   #remove stopwords \n",
    "    no_stop = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "    no_stopword = (' '.join(no_stop))\n",
    "        \n",
    "    #lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem = [lemmatizer.lemmatize(word) for word in no_stopword]\n",
    "    \n",
    "    #return words as a single string \n",
    "    return(''.join(lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking post_clean: \n",
      "['no new cluster beside two days', 'consistency accuracy could easier use data new moh situation report separates non foreigners citizen pr think count named clusters either linked construction sites live non cluster get categorized linked clusters pending investigations respectively moh situation report 28 3 10 4 ltp holders 545 linked clusters 126 linked clusters 141 pending investigations whereas estimate time period 702 construction related reason make graphs really 2 separate problems singapore circuit breaker slow growth general public stopping work construction sites circuit breaker doesn help construction site problem construction site problem tackled improving living conditions testing separating sick well conversely issues way many people exercising eating hawker centres sneakily meeting etc affect non problem yes testing situation worrisome not know prioritizing tests would political suicide moh admit either prioritizing sc pr first patriotism urgent separate sick cramped conditions', 'iq lower 86 believe csb maids pregnant nothing fws sex friends catholic believe immaculate inception like mary', 'work nearby westlite toh guan initial reports westlite saw still open operating usual even food stalls provision shop rather surprised didn try isolate situation earlier understand maybe couldn put everyone shn immediately financial implications wonder still allowed still congregate eating place also allowed provision shop stay open didn see sanitizing stuff going fair telecommuting week may done without seeing many decided lock place also spread toh guan across road couple units quite frustrated something wasn done earlier', 'ho seh']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_for_rnn</th>\n",
       "      <th>post_clean_nb_logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no new cluster beside the dorm for tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-10-04 08:34:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>there is no new cluster beside the dorm for tw...</td>\n",
       "      <td>no new cluster beside two days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;For consistency and accuracy, it could be eas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-11 12:26:38</td>\n",
       "      <td>reddit</td>\n",
       "      <td>for consistency and accuracy it could be easie...</td>\n",
       "      <td>consistency accuracy could easier use data new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only IQ lower than 86 will believe this CSB.Wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-23 10:42:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>only iq lower than 86 will believe this csb wh...</td>\n",
       "      <td>iq lower 86 believe csb maids pregnant nothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I work nearby to the Westlite and Toh Guan Dor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-06 20:55:47</td>\n",
       "      <td>reddit</td>\n",
       "      <td>i work nearby to the westlite and toh guan dor...</td>\n",
       "      <td>work nearby westlite toh guan initial reports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ho seh liao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-04 21:43:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>ho seh liao</td>\n",
       "      <td>ho seh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I’m not saying we caused this spread among the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-16 23:59:35</td>\n",
       "      <td>reddit</td>\n",
       "      <td>i m not saying we caused this spread among the...</td>\n",
       "      <td>not saying caused spread among agree oversight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1. From healthy no wear mask to Mask mandatory...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-14 20:29:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>1 from healthy no wear mask to mask mandatory ...</td>\n",
       "      <td>1 healthy no wear mask mask mandatory 2 many 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Exactly. People don’t even wanna let our publi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-05-27 20:28:30</td>\n",
       "      <td>reddit</td>\n",
       "      <td>exactly people don t even wanna let our public...</td>\n",
       "      <td>exactly people even wanna let public servants ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The current situation is beyond this woman. 24...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-06 14:40:00</td>\n",
       "      <td>sgtalk</td>\n",
       "      <td>the current situation is beyond this woman 24h...</td>\n",
       "      <td>current situation beyond woman 24hrs day not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iran, followed by China, India, Israel, Saudi ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-23 17:52:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>iran followed by china india israel saudi arab...</td>\n",
       "      <td>iran followed china india israel saudi arabia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0  There is no new cluster beside the dorm for tw...    2.0   \n",
       "1  >For consistency and accuracy, it could be eas...    0.0   \n",
       "2  Only IQ lower than 86 will believe this CSB.Wh...    0.0   \n",
       "3  I work nearby to the Westlite and Toh Guan Dor...    0.0   \n",
       "4                                        Ho seh liao    0.0   \n",
       "5  I’m not saying we caused this spread among the...    0.0   \n",
       "6  1. From healthy no wear mask to Mask mandatory...    0.0   \n",
       "7  Exactly. People don’t even wanna let our publi...    2.0   \n",
       "8  The current situation is beyond this woman. 24...    1.0   \n",
       "9  Iran, followed by China, India, Israel, Saudi ...    0.0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-10-04 08:34:00  hardwarezone   \n",
       "1  2020-04-11 12:26:38        reddit   \n",
       "2  2020-04-23 10:42:00  hardwarezone   \n",
       "3  2020-04-06 20:55:47        reddit   \n",
       "4  2020-09-04 21:43:00  hardwarezone   \n",
       "5  2020-04-16 23:59:35        reddit   \n",
       "6  2020-04-14 20:29:00  hardwarezone   \n",
       "7  2020-05-27 20:28:30        reddit   \n",
       "8  2020-06-06 14:40:00        sgtalk   \n",
       "9  2020-04-23 17:52:00  hardwarezone   \n",
       "\n",
       "                                  post_clean_for_rnn  \\\n",
       "0  there is no new cluster beside the dorm for tw...   \n",
       "1  for consistency and accuracy it could be easie...   \n",
       "2  only iq lower than 86 will believe this csb wh...   \n",
       "3  i work nearby to the westlite and toh guan dor...   \n",
       "4                                        ho seh liao   \n",
       "5  i m not saying we caused this spread among the...   \n",
       "6  1 from healthy no wear mask to mask mandatory ...   \n",
       "7  exactly people don t even wanna let our public...   \n",
       "8  the current situation is beyond this woman 24h...   \n",
       "9  iran followed by china india israel saudi arab...   \n",
       "\n",
       "                                post_clean_nb_logreg  \n",
       "0                     no new cluster beside two days  \n",
       "1  consistency accuracy could easier use data new...  \n",
       "2  iq lower 86 believe csb maids pregnant nothing...  \n",
       "3  work nearby westlite toh guan initial reports ...  \n",
       "4                                             ho seh  \n",
       "5  not saying caused spread among agree oversight...  \n",
       "6  1 healthy no wear mask mask mandatory 2 many 9...  \n",
       "7  exactly people even wanna let public servants ...  \n",
       "8  current situation beyond woman 24hrs day not e...  \n",
       "9  iran followed china india israel saudi arabia ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_clean = []\n",
    "\n",
    "for p in train[\"post\"]:\n",
    "    post_clean.append(preprocess(p))\n",
    "\n",
    "print(f\"checking post_clean: \\n{post_clean[0:5]}\")\n",
    "\n",
    "train['post_clean_nb_logreg'] = post_clean\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for test posts - for Recurrent Neural Network model \n",
    "\n",
    "Preprocessing for classification models - logreg and multinomial naive bayes already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not remove stop words to help with sequentiality \n",
    "\n",
    "def preprocess_rnn(word):\n",
    "     \n",
    "    # tokenize and convert lower \n",
    "    # \\w also removes punctuation - may need to add extra no punc if tokenizing does not do it\n",
    "    token = RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(word.lower())\n",
    " \n",
    "    #lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    #return words as a single string \n",
    "    return(' '.join(lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking post_clean_for_rnn: \n",
      "['im interested in the cluster esp new one where can i see', 'did the virus make his kkj buay kia']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean</th>\n",
       "      <th>post_clean_for_rnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need more camp to house all foreign worker</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-15 15:09:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>need camp house</td>\n",
       "      <td>need more camp to house all foreign worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im interested in the clusters esp new ones. Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-29 23:07:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>im interested clusters esp new ones see</td>\n",
       "      <td>im interested in the cluster esp new one where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did the virus make his kkj buay kia?</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-23 21:22:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>make kkj buay kia</td>\n",
       "      <td>did the virus make his kkj buay kia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m not saying there isn’t room for improvemen...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-08 11:23:23</td>\n",
       "      <td>reddit</td>\n",
       "      <td>not saying room improvement point choice nsfs ...</td>\n",
       "      <td>i m not saying there isn t room for improvemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnn good life no need work get paid and free ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-05 13:04:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>abnn good life no need work get paid free food...</td>\n",
       "      <td>abnn good life no need work get paid and free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have some information that some of these cb ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-14 20:23:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>information cb companies already pulled house ...</td>\n",
       "      <td>i have some information that some of these cb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A total of 122 are still unlinked, pending con...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-14 20:26:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>total 122 still unlinked pending contact traci...</td>\n",
       "      <td>a total of 122 are still unlinked pending cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Case 1121 Mr.Worldwide</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-04 21:53:14</td>\n",
       "      <td>reddit</td>\n",
       "      <td>case 1121 mr worldwide</td>\n",
       "      <td>case 1121 mr worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I expect another national broadcast in 1 weeks...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-04 19:20:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>expect another national broadcast 1 weeks time...</td>\n",
       "      <td>i expect another national broadcast in 1 week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Even if not govt's fault, they took their eyes...</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-04-17 16:56:00</td>\n",
       "      <td>sgtalk</td>\n",
       "      <td>even not govt fault took eyes ball not sure cl...</td>\n",
       "      <td>even if not govt s fault they took their eye o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0         Need more camp to house all foreign worker      1   \n",
       "1  Im interested in the clusters esp new ones. Wh...      1   \n",
       "2               did the virus make his kkj buay kia?      1   \n",
       "3  I’m not saying there isn’t room for improvemen...      1   \n",
       "4  abnn good life no need work get paid and free ...      1   \n",
       "5  I have some information that some of these cb ...      1   \n",
       "6  A total of 122 are still unlinked, pending con...      1   \n",
       "7                             Case 1121 Mr.Worldwide      1   \n",
       "8  I expect another national broadcast in 1 weeks...      1   \n",
       "9  Even if not govt's fault, they took their eyes...      0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-04-15 15:09:00  hardwarezone   \n",
       "1  2020-04-29 23:07:00  hardwarezone   \n",
       "2  2020-04-23 21:22:00  hardwarezone   \n",
       "3  2020-04-08 11:23:23        reddit   \n",
       "4  2020-04-05 13:04:00  hardwarezone   \n",
       "5  2020-04-14 20:23:00  hardwarezone   \n",
       "6  2020-04-14 20:26:00  hardwarezone   \n",
       "7  2020-04-04 21:53:14        reddit   \n",
       "8  2020-05-04 19:20:00  hardwarezone   \n",
       "9  2020-04-17 16:56:00        sgtalk   \n",
       "\n",
       "                                          post_clean  \\\n",
       "0                                    need camp house   \n",
       "1            im interested clusters esp new ones see   \n",
       "2                                  make kkj buay kia   \n",
       "3  not saying room improvement point choice nsfs ...   \n",
       "4  abnn good life no need work get paid free food...   \n",
       "5  information cb companies already pulled house ...   \n",
       "6  total 122 still unlinked pending contact traci...   \n",
       "7                             case 1121 mr worldwide   \n",
       "8  expect another national broadcast 1 weeks time...   \n",
       "9  even not govt fault took eyes ball not sure cl...   \n",
       "\n",
       "                                  post_clean_for_rnn  \n",
       "0         need more camp to house all foreign worker  \n",
       "1  im interested in the cluster esp new one where...  \n",
       "2                did the virus make his kkj buay kia  \n",
       "3  i m not saying there isn t room for improvemen...  \n",
       "4  abnn good life no need work get paid and free ...  \n",
       "5  i have some information that some of these cb ...  \n",
       "6  a total of 122 are still unlinked pending cont...  \n",
       "7                             case 1121 mr worldwide  \n",
       "8  i expect another national broadcast in 1 week ...  \n",
       "9  even if not govt s fault they took their eye o...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_clean_for_rnn = []\n",
    "\n",
    "for p in test[\"post\"]:\n",
    "    post_clean_for_rnn.append(preprocess_rnn(p))\n",
    "\n",
    "print(f\"checking post_clean_for_rnn: \\n{post_clean_for_rnn[1:3]}\")\n",
    "\n",
    "test['post_clean_for_rnn'] = post_clean_for_rnn\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2055, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_rnn</th>\n",
       "      <th>post_clean_nb_logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need more camp to house all foreign worker</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-15 15:09:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>need more camp to house all foreign worker</td>\n",
       "      <td>need camp house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im interested in the clusters esp new ones. Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-29 23:07:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>im interested in the cluster esp new one where...</td>\n",
       "      <td>im interested clusters esp new ones see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0         Need more camp to house all foreign worker      1   \n",
       "1  Im interested in the clusters esp new ones. Wh...      1   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-04-15 15:09:00  hardwarezone   \n",
       "1  2020-04-29 23:07:00  hardwarezone   \n",
       "\n",
       "                                      post_clean_rnn  \\\n",
       "0         need more camp to house all foreign worker   \n",
       "1  im interested in the cluster esp new one where...   \n",
       "\n",
       "                      post_clean_nb_logreg  \n",
       "0                          need camp house  \n",
       "1  im interested clusters esp new ones see  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming and reordering columns in test set \n",
    "\n",
    "test.rename(columns = {'post_clean': 'post_clean_nb_logreg',\n",
    "                      'post_clean_for_rnn': 'post_clean_rnn'}, inplace=True)\n",
    "test = test[['post', 'label', 'date', 'source', 'post_clean_rnn', 'post_clean_nb_logreg']]\n",
    "display(test.shape)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8213, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_rnn</th>\n",
       "      <th>post_clean_nb_logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no new cluster beside the dorm for tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-10-04 08:34:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>there is no new cluster beside the dorm for tw...</td>\n",
       "      <td>no new cluster beside two days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;For consistency and accuracy, it could be eas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-11 12:26:38</td>\n",
       "      <td>reddit</td>\n",
       "      <td>for consistency and accuracy it could be easie...</td>\n",
       "      <td>consistency accuracy could easier use data new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0  There is no new cluster beside the dorm for tw...    2.0   \n",
       "1  >For consistency and accuracy, it could be eas...    0.0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-10-04 08:34:00  hardwarezone   \n",
       "1  2020-04-11 12:26:38        reddit   \n",
       "\n",
       "                                      post_clean_rnn  \\\n",
       "0  there is no new cluster beside the dorm for tw...   \n",
       "1  for consistency and accuracy it could be easie...   \n",
       "\n",
       "                                post_clean_nb_logreg  \n",
       "0                     no new cluster beside two days  \n",
       "1  consistency accuracy could easier use data new...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming columns in train set \n",
    "train.rename(columns = {'post_clean_for_rnn': 'post_clean_rnn'}, inplace=True)\n",
    "display(train.shape)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10268, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>post_clean_rnn</th>\n",
       "      <th>post_clean_nb_logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no new cluster beside the dorm for tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-10-04 08:34:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>there is no new cluster beside the dorm for tw...</td>\n",
       "      <td>no new cluster beside two days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;For consistency and accuracy, it could be eas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-11 12:26:38</td>\n",
       "      <td>reddit</td>\n",
       "      <td>for consistency and accuracy it could be easie...</td>\n",
       "      <td>consistency accuracy could easier use data new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only IQ lower than 86 will believe this CSB.Wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-23 10:42:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>only iq lower than 86 will believe this csb wh...</td>\n",
       "      <td>iq lower 86 believe csb maids pregnant nothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I work nearby to the Westlite and Toh Guan Dor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-06 20:55:47</td>\n",
       "      <td>reddit</td>\n",
       "      <td>i work nearby to the westlite and toh guan dor...</td>\n",
       "      <td>work nearby westlite toh guan initial reports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ho seh liao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-04 21:43:00</td>\n",
       "      <td>hardwarezone</td>\n",
       "      <td>ho seh liao</td>\n",
       "      <td>ho seh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  label  \\\n",
       "0  There is no new cluster beside the dorm for tw...    2.0   \n",
       "1  >For consistency and accuracy, it could be eas...    0.0   \n",
       "2  Only IQ lower than 86 will believe this CSB.Wh...    0.0   \n",
       "3  I work nearby to the Westlite and Toh Guan Dor...    0.0   \n",
       "4                                        Ho seh liao    0.0   \n",
       "\n",
       "                  date        source  \\\n",
       "0  2020-10-04 08:34:00  hardwarezone   \n",
       "1  2020-04-11 12:26:38        reddit   \n",
       "2  2020-04-23 10:42:00  hardwarezone   \n",
       "3  2020-04-06 20:55:47        reddit   \n",
       "4  2020-09-04 21:43:00  hardwarezone   \n",
       "\n",
       "                                      post_clean_rnn  \\\n",
       "0  there is no new cluster beside the dorm for tw...   \n",
       "1  for consistency and accuracy it could be easie...   \n",
       "2  only iq lower than 86 will believe this csb wh...   \n",
       "3  i work nearby to the westlite and toh guan dor...   \n",
       "4                                        ho seh liao   \n",
       "\n",
       "                                post_clean_nb_logreg  \n",
       "0                     no new cluster beside two days  \n",
       "1  consistency accuracy could easier use data new...  \n",
       "2  iq lower 86 believe csb maids pregnant nothing...  \n",
       "3  work nearby westlite toh guan initial reports ...  \n",
       "4                                             ho seh  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts = pd.concat([train, test], axis = 0, ignore_index=True)\n",
    "display(all_posts.shape)\n",
    "all_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    8907\n",
       "0.0    1120\n",
       "2.0     241\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export allposts \n",
    "\n",
    "all_posts.to_csv('./labelled_posts/all_posts_labelled.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating performance of RNN model on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17458 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize post text, by turning each text into either a sequence of integers or into a vector\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "max_nb_words = 50000\n",
    "\n",
    "# Max number of words in each complaint.\n",
    "max_seq_length = 512\n",
    "\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(all_posts['post_clean_rnn'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10268, 512)\n"
     ]
    }
   ],
   "source": [
    "#truncate and pad \n",
    "X = tokenizer.texts_to_sequences(all_posts['post_clean_rnn'].values)\n",
    "X = pad_sequences(X, maxlen=max_seq_length)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10268, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding labels \n",
    "y = pd.get_dummies(all_posts['label']).values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8214, 512) (8214, 3)\n",
      "(2054, 512) (2054, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2, \n",
    "                                                 stratify = y, random_state = 42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 512, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 512, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 5,101,779\n",
      "Trainable params: 5,101,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, embedding_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "257/257 [==============================] - 656s 3s/step - loss: 0.4759 - auc: 0.9203 - val_loss: 0.4523 - val_auc: 0.9265\n",
      "Epoch 2/20\n",
      "257/257 [==============================] - 710s 3s/step - loss: 0.4742 - auc: 0.9208 - val_loss: 0.4573 - val_auc: 0.9248\n",
      "Epoch 3/20\n",
      "257/257 [==============================] - 629s 2s/step - loss: 0.4700 - auc: 0.9217 - val_loss: 0.4526 - val_auc: 0.9259\n",
      "Epoch 4/20\n",
      "257/257 [==============================] - 729s 3s/step - loss: 0.4695 - auc: 0.9218 - val_loss: 0.4538 - val_auc: 0.9243\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 22s 336ms/step - loss: 0.4538 - auc: 0.9243\n",
      "Test set\n",
      "  Loss: 0.454\n",
      "  Accuracy: 0.924\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = model.evaluate(X_val,y_val)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  ROC AUC: {:0.3f}'.format(acc[0],acc[1]))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating performance of Logistic Regression Model on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_posts['post_clean_nb_logreg']\n",
    "y = all_posts['label']\n",
    "\n",
    "#train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.2, 0.3, 0.7],\n",
       "                         'cvec__max_features': [100, 200, 500],\n",
       "                         'cvec__min_df': [2, 4, 6],\n",
       "                         'cvec__ngram_range': [(1, 1)],\n",
       "                         'lr__C': array([1.00000000e-05, 4.64158883e-05, 2.15443469e-04, 1.00000000e-03,\n",
       "       4.64158883e-03, 2.15443469e-02, 1.00000000e-01, 4.64158883e-01,\n",
       "       2.15443469e+00, 1.00000000e+01]),\n",
       "                         'lr__penalty': ['l1']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [100, 200, 500],\n",
    "    'cvec__min_df': [2, 4, 6],\n",
    "    'cvec__max_df': [0.2, 0.3, 0.7],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'lr__penalty': ['l1'],\n",
    "    'lr__C': np.logspace(-5, 1, 10)\n",
    "}\n",
    "\n",
    "gscv_lr = GridSearchCV(pipe, pipe_params, cv=5, n_jobs =-1, verbose=1)\n",
    "gscv_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': 200,\n",
       " 'cvec__min_df': 4,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'lr__C': 0.1,\n",
       " 'lr__penalty': 'l1'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, max_features=200, min_df=4)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model with optimised params \n",
    "opt_gscv_lr = gscv_lr.best_estimator_\n",
    "opt_gscv_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimised_params</th>\n",
       "      <th>train_acc_score</th>\n",
       "      <th>test_acc_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec + logistic regression</td>\n",
       "      <td>{'cvec__max_df': 0.2, 'cvec__max_features': 20...</td>\n",
       "      <td>0.874604</td>\n",
       "      <td>0.872931</td>\n",
       "      <td>0.785813</td>\n",
       "      <td>0.916755</td>\n",
       "      <td>0.914799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  \\\n",
       "0  cvec + logistic regression   \n",
       "\n",
       "                                    optimised_params  train_acc_score  \\\n",
       "0  {'cvec__max_df': 0.2, 'cvec__max_features': 20...         0.874604   \n",
       "\n",
       "   test_acc_score  roc_auc_score  train_f1_score  test_f1_score  \n",
       "0        0.872931       0.785813        0.916755       0.914799  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "      <th>pred 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>25</td>\n",
       "      <td>197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>14</td>\n",
       "      <td>1767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 2</th>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pred 0  pred 1  pred 2\n",
       "actual 0      25     197       2\n",
       "actual 1      14    1767       1\n",
       "actual 2       8      39       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.532     0.112     0.185       224\n",
      "         1.0      0.882     0.992     0.934      1782\n",
      "         2.0      0.250     0.021     0.038        48\n",
      "\n",
      "    accuracy                          0.873      2054\n",
      "   macro avg      0.555     0.375     0.386      2054\n",
      "weighted avg      0.829     0.873     0.831      2054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataframe of performance metrics based on optimised model \n",
    "opt_results_lr = pd.DataFrame()\n",
    "\n",
    "opt_results_lr['model'] = ['cvec + logistic regression']\n",
    "opt_results_lr['optimised_params'] = [gscv_lr.best_params_]\n",
    "opt_results_lr['train_acc_score'] = opt_gscv_lr.score(X_train, y_train)\n",
    "opt_results_lr['test_acc_score'] = opt_gscv_lr.score(X_test, y_test)\n",
    "\n",
    "pred_proba = opt_gscv_lr.predict_proba(X_test)\n",
    "opt_results_lr['roc_auc_score'] = roc_auc_score(y_test, pred_proba, multi_class=\"ovo\", average = 'weighted')\n",
    "opt_results_lr['train_f1_score'] = f1_score((opt_gscv_lr.predict(X_train)), y_train, average = 'weighted')\n",
    "opt_results_lr['test_f1_score'] = f1_score((opt_gscv_lr.predict(X_test)), y_test, average = 'weighted')\n",
    "\n",
    "display(opt_results_lr)\n",
    "\n",
    "#confusion matrix for logreg\n",
    "\n",
    "y_pred_lr = opt_gscv_lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['actual 0','actual 1','actual 2'], \n",
    "                     columns = ['pred 0','pred 1','pred 2'])\n",
    "display(cm_df)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating performance of Multinomial Naive Bayes Model on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   22.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.2, 0.3, 0.5, 0.7],\n",
       "                         'cvec__max_features': [500], 'cvec__min_df': [2, 4, 6],\n",
       "                         'cvec__ngram_range': [(1, 2)]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [500],\n",
    "    'cvec__min_df': [2, 4, 6],\n",
    "    'cvec__max_df': [0.2, 0.3, 0.5, 0.7],\n",
    "    'cvec__ngram_range': [(1,2)]}\n",
    "\n",
    "\n",
    "gscv_nb = GridSearchCV(pipe, pipe_params, cv=5, n_jobs =-1, verbose=1)\n",
    "gscv_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': 500,\n",
       " 'cvec__min_df': 4,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, max_features=500, min_df=4,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model with optimised params \n",
    "opt_gscv_nb = gscv_nb.best_estimator_\n",
    "opt_gscv_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimised_params</th>\n",
       "      <th>train_acc_score</th>\n",
       "      <th>test_acc_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec + multinomial nb</td>\n",
       "      <td>{'cvec__max_df': 0.2, 'cvec__max_features': 50...</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.869036</td>\n",
       "      <td>0.727923</td>\n",
       "      <td>0.875525</td>\n",
       "      <td>0.877433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model                                   optimised_params  \\\n",
       "0  cvec + multinomial nb  {'cvec__max_df': 0.2, 'cvec__max_features': 50...   \n",
       "\n",
       "   train_acc_score  test_acc_score  roc_auc_score  train_f1_score  \\\n",
       "0           0.8701        0.869036       0.727923        0.875525   \n",
       "\n",
       "   test_f1_score  \n",
       "0       0.877433  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "      <th>pred 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>80</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>65</td>\n",
       "      <td>1692</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 2</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pred 0  pred 1  pred 2\n",
       "actual 0      80     133      11\n",
       "actual 1      65    1692      25\n",
       "actual 2      10      25      13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.516     0.357     0.422       224\n",
      "         1.0      0.915     0.949     0.932      1782\n",
      "         2.0      0.265     0.271     0.268        48\n",
      "\n",
      "    accuracy                          0.869      2054\n",
      "   macro avg      0.565     0.526     0.541      2054\n",
      "weighted avg      0.856     0.869     0.861      2054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataframe of metrics based on optimised model \n",
    "opt_results_nb = pd.DataFrame()\n",
    "\n",
    "opt_results_nb['model'] = ['cvec + multinomial nb']\n",
    "opt_results_nb['optimised_params'] = [gscv_nb.best_params_]\n",
    "opt_results_nb['train_acc_score'] = opt_gscv_nb.score(X_train, y_train)\n",
    "opt_results_nb['test_acc_score'] = opt_gscv_nb.score(X_test, y_test)\n",
    "\n",
    "pred_proba = opt_gscv_nb.predict_proba(X_test)\n",
    "opt_results_nb['roc_auc_score'] = roc_auc_score(y_test, pred_proba, multi_class=\"ovo\", average = 'weighted')\n",
    "opt_results_nb['train_f1_score'] = f1_score((opt_gscv_nb.predict(X_train)), y_train, average = 'weighted')\n",
    "opt_results_nb['test_f1_score'] = f1_score((opt_gscv_nb.predict(X_test)), y_test, average = 'weighted')\n",
    "\n",
    "display(opt_results_nb)\n",
    "\n",
    "#confusion matrix and classification report for multinomial nb \n",
    "\n",
    "y_pred_nb = opt_gscv_nb.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['actual 0','actual 1','actual 2'], \n",
    "                     columns = ['pred 0','pred 1','pred 2'])\n",
    "display(cm_df)\n",
    "print('')\n",
    "print(classification_report(y_test, y_pred_nb, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Production Model\n",
    "\n",
    "Chose multinomial naive bayes model - low variance for accuracy scores and f1-scores, predicted minority classes more accurately than logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.2, max_features=500, min_df=2)),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_model = opt_gscv_nb.fit(X, y)\n",
    "production_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting coefficients from Multinomial NB model \n",
    "\n",
    "Upside of using a classification model instead of neural network - interpretability. \n",
    "\n",
    "Words with smallest negative coefficient values have the LEAST IMPACT in lowering the probability of the class, more likely that a word indicates negative, neutral or positive sentiment.\n",
    "\n",
    "Words with largest negative coefficient values have the MOST IMPACT in lowering the probability of the class, less likely that a word indicates negative, neutral or positive sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef_negative_class</th>\n",
       "      <th>coef_neutral_class</th>\n",
       "      <th>coef_positive_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "      <td>-6.169192</td>\n",
       "      <td>-5.618496</td>\n",
       "      <td>-6.053970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-6.113622</td>\n",
       "      <td>-5.880141</td>\n",
       "      <td>-6.970260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>-6.780101</td>\n",
       "      <td>-6.630997</td>\n",
       "      <td>-6.816110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>-7.778630</td>\n",
       "      <td>-6.746308</td>\n",
       "      <td>-7.375726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>-7.778630</td>\n",
       "      <td>-6.827277</td>\n",
       "      <td>-8.762020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  coef_negative_class  coef_neutral_class  coef_positive_class\n",
       "0  000            -6.169192           -5.618496            -6.053970\n",
       "1   10            -6.113622           -5.880141            -6.970260\n",
       "2  100            -6.780101           -6.630997            -6.816110\n",
       "3   12            -7.778630           -6.746308            -7.375726\n",
       "4  120            -7.778630           -6.827277            -8.762020"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = opt_gscv_nb.named_steps['nb'].coef_\n",
    "display(coefs.shape)\n",
    "\n",
    "features = opt_gscv_nb.named_steps['cvec'].get_feature_names()\n",
    "display(len(features))\n",
    "\n",
    "#creating dataframe of words and coefficients for each class\n",
    "coef_df = pd.DataFrame({'word':features, \n",
    "                        'coef_negative_class':coefs[0], \n",
    "                        'coef_neutral_class':coefs[1], \n",
    "                        'coef_positive_class':coefs[2]})\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative class words not too dissimilar from positive class words. \n",
    "\n",
    "Exceptions are calling out organisations and agencies in their posts - 'PAP' (People's Action Party - the ruling party) and 'MOM' (Ministry of Manpower), perhaps \n",
    "\n",
    "The word 'still' indicates a sentiment in these posts that COVID-19 cases in dorms remain high. \n",
    "\n",
    "'Mask' is also another word in negative sentiment posts - perhaps taking issue with the Government's delay in asking people to wear masks in public. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef_negative_class</th>\n",
       "      <th>coef_neutral_class</th>\n",
       "      <th>coef_positive_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>no</td>\n",
       "      <td>-4.156196</td>\n",
       "      <td>-4.352461</td>\n",
       "      <td>-4.355301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>like</td>\n",
       "      <td>-4.451541</td>\n",
       "      <td>-4.491513</td>\n",
       "      <td>-4.527913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>government</td>\n",
       "      <td>-4.493432</td>\n",
       "      <td>-5.572794</td>\n",
       "      <td>-4.367571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>pap</td>\n",
       "      <td>-4.493432</td>\n",
       "      <td>-6.394087</td>\n",
       "      <td>-5.989431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>people</td>\n",
       "      <td>-4.542757</td>\n",
       "      <td>-4.515099</td>\n",
       "      <td>-4.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>one</td>\n",
       "      <td>-4.618595</td>\n",
       "      <td>-4.595838</td>\n",
       "      <td>-4.667675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>should</td>\n",
       "      <td>-4.636944</td>\n",
       "      <td>-4.909292</td>\n",
       "      <td>-4.572365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>mom</td>\n",
       "      <td>-4.775780</td>\n",
       "      <td>-6.081331</td>\n",
       "      <td>-6.816110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>need</td>\n",
       "      <td>-4.782898</td>\n",
       "      <td>-4.821248</td>\n",
       "      <td>-5.206672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>said</td>\n",
       "      <td>-4.834191</td>\n",
       "      <td>-4.620725</td>\n",
       "      <td>-4.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>singapore</td>\n",
       "      <td>-4.912356</td>\n",
       "      <td>-4.432673</td>\n",
       "      <td>-4.471561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>still</td>\n",
       "      <td>-4.928750</td>\n",
       "      <td>-4.752364</td>\n",
       "      <td>-5.206672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>work</td>\n",
       "      <td>-4.953856</td>\n",
       "      <td>-4.855877</td>\n",
       "      <td>-5.429815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>govt</td>\n",
       "      <td>-4.953856</td>\n",
       "      <td>-6.104454</td>\n",
       "      <td>-5.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>also</td>\n",
       "      <td>-4.962366</td>\n",
       "      <td>-4.690760</td>\n",
       "      <td>-4.471561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>see</td>\n",
       "      <td>-4.979608</td>\n",
       "      <td>-5.132435</td>\n",
       "      <td>-5.265512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>many</td>\n",
       "      <td>-4.997152</td>\n",
       "      <td>-4.865995</td>\n",
       "      <td>-4.810776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>even</td>\n",
       "      <td>-5.015010</td>\n",
       "      <td>-4.990367</td>\n",
       "      <td>-4.911872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>go</td>\n",
       "      <td>-5.051711</td>\n",
       "      <td>-4.798811</td>\n",
       "      <td>-5.124434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>time</td>\n",
       "      <td>-5.051711</td>\n",
       "      <td>-4.988450</td>\n",
       "      <td>-5.151102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  coef_negative_class  coef_neutral_class  coef_positive_class\n",
       "292          no            -4.156196           -4.352461            -4.355301\n",
       "234        like            -4.451541           -4.491513            -4.527913\n",
       "170  government            -4.493432           -5.572794            -4.367571\n",
       "311         pap            -4.493432           -6.394087            -5.989431\n",
       "319      people            -4.542757           -4.515099            -4.197672\n",
       "301         one            -4.618595           -4.595838            -4.667675\n",
       "388      should            -4.636944           -4.909292            -4.572365\n",
       "276         mom            -4.775780           -6.081331            -6.816110\n",
       "286        need            -4.782898           -4.821248            -5.206672\n",
       "372        said            -4.834191           -4.620725            -4.870200\n",
       "392   singapore            -4.912356           -4.432673            -4.471561\n",
       "416       still            -4.928750           -4.752364            -5.206672\n",
       "487        work            -4.953856           -4.855877            -5.429815\n",
       "171        govt            -4.953856           -6.104454            -5.000820\n",
       "20         also            -4.962366           -4.690760            -4.471561\n",
       "378         see            -4.979608           -5.132435            -5.265512\n",
       "258        many            -4.997152           -4.865995            -4.810776\n",
       "132        even            -5.015010           -4.990367            -4.911872\n",
       "163          go            -5.051711           -4.798811            -5.124434\n",
       "445        time            -5.051711           -4.988450            -5.151102"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by ='coef_negative_class',ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words indicating positive classes\n",
    "\n",
    "Words like 'people', 'government', 'singapore' are top words indicating positive sentiment - similar to negative sentiment coefficients but of more importance\n",
    "\n",
    "'Would', 'should' - indicating some kind of suggestion in the post, hence will do topic modelling in next section\n",
    "\n",
    "'Good' also in top words, but not a very large negative coefficient, suggestion other words are more prominent in positive sentiment posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef_negative_class</th>\n",
       "      <th>coef_neutral_class</th>\n",
       "      <th>coef_positive_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>people</td>\n",
       "      <td>-4.542757</td>\n",
       "      <td>-4.515099</td>\n",
       "      <td>-4.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>no</td>\n",
       "      <td>-4.156196</td>\n",
       "      <td>-4.352461</td>\n",
       "      <td>-4.355301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>government</td>\n",
       "      <td>-4.493432</td>\n",
       "      <td>-5.572794</td>\n",
       "      <td>-4.367571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>also</td>\n",
       "      <td>-4.962366</td>\n",
       "      <td>-4.690760</td>\n",
       "      <td>-4.471561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>would</td>\n",
       "      <td>-5.149829</td>\n",
       "      <td>-4.967596</td>\n",
       "      <td>-4.471561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>singapore</td>\n",
       "      <td>-4.912356</td>\n",
       "      <td>-4.432673</td>\n",
       "      <td>-4.471561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>like</td>\n",
       "      <td>-4.451541</td>\n",
       "      <td>-4.491513</td>\n",
       "      <td>-4.527913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>should</td>\n",
       "      <td>-4.636944</td>\n",
       "      <td>-4.909292</td>\n",
       "      <td>-4.572365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>think</td>\n",
       "      <td>-5.080149</td>\n",
       "      <td>-4.803577</td>\n",
       "      <td>-4.603137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>one</td>\n",
       "      <td>-4.618595</td>\n",
       "      <td>-4.595838</td>\n",
       "      <td>-4.667675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>many</td>\n",
       "      <td>-4.997152</td>\n",
       "      <td>-4.865995</td>\n",
       "      <td>-4.810776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>said</td>\n",
       "      <td>-4.834191</td>\n",
       "      <td>-4.620725</td>\n",
       "      <td>-4.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>even</td>\n",
       "      <td>-5.015010</td>\n",
       "      <td>-4.990367</td>\n",
       "      <td>-4.911872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>numbers</td>\n",
       "      <td>-6.086954</td>\n",
       "      <td>-5.607763</td>\n",
       "      <td>-4.933379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>say</td>\n",
       "      <td>-5.119370</td>\n",
       "      <td>-5.316528</td>\n",
       "      <td>-4.977830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>govt</td>\n",
       "      <td>-4.953856</td>\n",
       "      <td>-6.104454</td>\n",
       "      <td>-5.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>know</td>\n",
       "      <td>-5.149829</td>\n",
       "      <td>-5.249665</td>\n",
       "      <td>-5.073140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>situation</td>\n",
       "      <td>-5.393807</td>\n",
       "      <td>-5.838946</td>\n",
       "      <td>-5.098458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>masks</td>\n",
       "      <td>-5.342514</td>\n",
       "      <td>-6.209165</td>\n",
       "      <td>-5.098458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>good</td>\n",
       "      <td>-5.342514</td>\n",
       "      <td>-5.382486</td>\n",
       "      <td>-5.098458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  coef_negative_class  coef_neutral_class  coef_positive_class\n",
       "319      people            -4.542757           -4.515099            -4.197672\n",
       "292          no            -4.156196           -4.352461            -4.355301\n",
       "170  government            -4.493432           -5.572794            -4.367571\n",
       "20         also            -4.962366           -4.690760            -4.471561\n",
       "493       would            -5.149829           -4.967596            -4.471561\n",
       "392   singapore            -4.912356           -4.432673            -4.471561\n",
       "234        like            -4.451541           -4.491513            -4.527913\n",
       "388      should            -4.636944           -4.909292            -4.572365\n",
       "440       think            -5.080149           -4.803577            -4.603137\n",
       "301         one            -4.618595           -4.595838            -4.667675\n",
       "258        many            -4.997152           -4.865995            -4.810776\n",
       "372        said            -4.834191           -4.620725            -4.870200\n",
       "132        even            -5.015010           -4.990367            -4.911872\n",
       "297     numbers            -6.086954           -5.607763            -4.933379\n",
       "374         say            -5.119370           -5.316528            -4.977830\n",
       "171        govt            -4.953856           -6.104454            -5.000820\n",
       "219        know            -5.149829           -5.249665            -5.073140\n",
       "398   situation            -5.393807           -5.838946            -5.098458\n",
       "260       masks            -5.342514           -6.209165            -5.098458\n",
       "167        good            -5.342514           -5.382486            -5.098458"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by ='coef_positive_class',ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of negative/neutral/positive sentiment posts per source \n",
    "\n",
    "#### Reddit\n",
    "- Negative sentiment posts: 247 of 3493 posts (7.08%)\n",
    "- Neutral sentiment posts: 3109 of 3493 posts (89%)\n",
    "- Positive sentiment posts 137 of 3493 posts (3.92%)\n",
    "\n",
    "#### Hardwarezone \n",
    "- Negative sentiment posts: 673 of 6081 posts (11%)\n",
    "- Neutral sentiment posts: 5318 of 6081 posts (87.5%)\n",
    "- Positive sentiment posts: 90 of 6081 posts (1.5%)\n",
    "\n",
    "#### Sgtalk\n",
    "- Negative sentiment posts: 200 of 694 posts (28%)\n",
    "- Neutral sentiment posts: 480 of 694 posts (70%)\n",
    "- Positive sentiment posts 14 of 694 posts (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hardwarezone    6081\n",
       "reddit          3493\n",
       "sgtalk           694\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********REDDIT**********\n",
      "\n",
      "reddit posts with negative sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    247\n",
       "label                   247\n",
       "date                    247\n",
       "source                  247\n",
       "post_clean_rnn          247\n",
       "post_clean_nb_logreg    247\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit posts with neutral sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    3109\n",
       "label                   3109\n",
       "date                    3109\n",
       "source                  3109\n",
       "post_clean_rnn          3109\n",
       "post_clean_nb_logreg    3109\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit posts with positive sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    137\n",
       "label                   137\n",
       "date                    137\n",
       "source                  137\n",
       "post_clean_rnn          137\n",
       "post_clean_nb_logreg    137\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********HARDWAREZONE**********\n",
      "\n",
      "hardwarezone posts with negative sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    673\n",
       "label                   673\n",
       "date                    673\n",
       "source                  673\n",
       "post_clean_rnn          673\n",
       "post_clean_nb_logreg    673\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwarezone posts with neutral sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    5318\n",
       "label                   5318\n",
       "date                    5318\n",
       "source                  5318\n",
       "post_clean_rnn          5318\n",
       "post_clean_nb_logreg    5318\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwarezone posts with positive sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    90\n",
       "label                   90\n",
       "date                    90\n",
       "source                  90\n",
       "post_clean_rnn          90\n",
       "post_clean_nb_logreg    90\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********SGTALK**********\n",
      "\n",
      "sgtalk posts with negative sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    200\n",
       "label                   200\n",
       "date                    200\n",
       "source                  200\n",
       "post_clean_rnn          200\n",
       "post_clean_nb_logreg    200\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgtalk posts with neutral sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    480\n",
       "label                   480\n",
       "date                    480\n",
       "source                  480\n",
       "post_clean_rnn          480\n",
       "post_clean_nb_logreg    480\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgtalk posts with positive sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorafoong/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post                    14\n",
       "label                   14\n",
       "date                    14\n",
       "source                  14\n",
       "post_clean_rnn          14\n",
       "post_clean_nb_logreg    14\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#positive/neutral/neg posts skewed to which source?\n",
    "\n",
    "#REDDIT\n",
    "print('**********REDDIT**********')\n",
    "print('')\n",
    "print('reddit posts with negative sentiment')\n",
    "display(all_posts[all_posts['label'] == 0][all_posts['source'] == 'reddit'].count())\n",
    "\n",
    "print('reddit posts with neutral sentiment')\n",
    "display(all_posts[all_posts['label'] == 1][all_posts['source'] == 'reddit'].count())\n",
    "\n",
    "print('reddit posts with positive sentiment')\n",
    "display(all_posts[all_posts['label'] == 2][all_posts['source'] == 'reddit'].count())\n",
    "\n",
    "#HARDWAREZONE\n",
    "print('**********HARDWAREZONE**********')\n",
    "print('')\n",
    "print('hardwarezone posts with negative sentiment')\n",
    "display(all_posts[all_posts['label'] == 0][all_posts['source'] == 'hardwarezone'].count())\n",
    "\n",
    "print('hardwarezone posts with neutral sentiment')\n",
    "display(all_posts[all_posts['label'] == 1][all_posts['source'] == 'hardwarezone'].count())\n",
    "\n",
    "print('hardwarezone posts with positive sentiment')\n",
    "display(all_posts[all_posts['label'] == 2][all_posts['source'] == 'hardwarezone'].count())\n",
    "\n",
    "#SGTALK\n",
    "print('**********SGTALK**********')\n",
    "print('')\n",
    "print('sgtalk posts with negative sentiment')\n",
    "display(all_posts[all_posts['label'] == 0][all_posts['source'] == 'sgtalk'].count())\n",
    "\n",
    "print('sgtalk posts with neutral sentiment')\n",
    "display(all_posts[all_posts['label'] == 1][all_posts['source'] == 'sgtalk'].count())\n",
    "\n",
    "print('sgtalk posts with positive sentiment')\n",
    "display(all_posts[all_posts['label'] == 2][all_posts['source'] == 'sgtalk'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-10-04 08:34:00', '2020-04-11 12:26:38',\n",
       "       '2020-04-23 10:42:00', ..., '2020-11-05 22:17:00',\n",
       "       '2020-10-04 08:30:00', '2020-04-14 13:34:14'], dtype=object)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of positive/neg posts evolved over time? \n",
    "# unable to generate insights as many post dates were incorrectly labelled by the source \n",
    "\n",
    "all_posts['date'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
